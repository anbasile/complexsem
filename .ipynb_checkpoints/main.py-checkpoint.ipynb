{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Computational Lexical Semantics</center></h1>\n",
    "<h5><center>LIN5580-SEM1-A-1718</center></h5>\n",
    "<h3><center>Angelo Basile and Jovana Urosevic</center></h3>\n",
    "<h5><center>January 31, 2018</center></h5>\n",
    "\n",
    "-----\n",
    "### Instruction on how to use this notebook to replicate the results:\n",
    "\n",
    "1. download and install python 2\n",
    "2. download and install [pipenv](https://docs.pipenv.org/)\n",
    "3. install cython separately: ```pipenv install cython```\n",
    "4. install the requirements from the Pipfile\n",
    "5. active the virtualenvironment: ```pipenv shell```\n",
    "6. ```cd dissect-master```\n",
    "7. install dissect: ```pipenv run python2 setup.py install```\n",
    "8. deactive the virtual environment:```exit```\n",
    "9. run the notebook: ```pipenv run jupyter notebook```\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that dissect is installed correctly:\n",
    "# this import should give no errors\n",
    "from composes.semantic_space.space import Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Describe the Distributional Hypothesis and explain how it can be used to find semantically similar words in corpora. (3 points)\n",
    "\n",
    "Distributional hypothesis was introduced by Harris (1968) and refers to the relation between the word meaning and the context it appears in (that is, the distribution of other words that accompany it and/or their syntactic relations) (Jurafsky and Martin, 2017). If we can understand the word meaning from the context it is commonly used in, then the words with similar meaning should be very likely to appear in similar contexts (ibid.).  \n",
    "&ensp;&ensp; In order to see how this works in practice, we can look at the example below:\n",
    "\n",
    "(1) &ensp;*The woolly **lemur**, the aye aye and the ring tail **lemur** can be found on the island of Madagascar.  \n",
    "&ensp;&ensp;&ensp; **Lemurs** are best known for their large, round reflective eyes and their wailing screams.  \n",
    "&ensp;&ensp;&ensp; The biggest threat to the **lemur** is deforestation.*  \n",
    "\n",
    "If we did not know what the word *lemur* was, just based on these sentences we could find out that it is an animal that lives on Madagascar, that has big, round eyes and loud screams, that it probably lives in the trees, etc. Besides this, if we take a look at the concordance view in the corpus (example 2), not only will we see other contexts that explain the word *lemur*, but also other similar words such as monkey, primate, species, etc. that frequently appear with it.\n",
    "\n",
    "(2)\n",
    "*&ensp;&ensp; -xxnigeoo-langren-20101250the lemur <font color=red>monkey</font> or &ensp;&ensp;&ensp; **lemur** &ensp;&ensp;&ensp;&ensp;&ensp; is surely an unique <font color=red>animal</font> that is native  \n",
    "&ensp;&ensp;&ensp;&ensp; forests, and provides homes for red ruffed &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; &ensp; **lemur** &ensp;&ensp;&ensp;&ensp;&ensp; , <font color=red>gorillas, colobus monkeys</font>, and <font color=red>jaguars</font>  \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp; Concreting the area in front of the new &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **lemur** &ensp;&ensp;&ensp;&ensp;&ensp; enclosure we could have enough done over  \n",
    "&ensp;&ensp;&ensp;&ensp; so now at gymnastics my ten year old is a &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;   **lemur** &ensp;&ensp;&ensp;&ensp;&ensp; leaping a <font color=red>gcheetah</font> chasing a <font color=red>spider</font> spanning  \n",
    "&ensp;&ensp;&ensp;&ensp; <font color=red>gelephants, tigers (cubs), zebras, snakes</font>,  &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **lemurs** &ensp;&ensp;&ensp;&ensp; <font color=red>gmonkeys, birds, kangaroos, ostriches</font>,  \n",
    "&ensp;&ensp;&ensp;&ensp; Really feel distrusted and alienated The &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; &ensp;   **lemur** &ensp;&ensp;&ensp;&ensp;&ensp; <font color=red>monkey</font> is called so simply because it is  \n",
    "&ensp;&ensp;&ensp;&ensp; Live here including 90% of the world’s the &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;   **lemur** &ensp;&ensp;&ensp;&ensp;&ensp;<font color=red>monkeys</font> . Madagascar is made up of*  \n",
    "\n",
    "The fact that we can frequently see other words naming types of animals around the word *lemur*, points to a kind of a relation these words have with our word *lemur*. And if we look the word *gorilla* up in the corpus (example 3), we will again get quite similar words around it in the context when it is used to refer to an animal (and not as an attribute for a person): *monkey, chimpanzee, animal, species,* with the description of where and how it lives, etc. So quite similar to the previous example. And this is what distributional hypothesis is about - we can understand the meaning of a word from the context, and we can find other similar words by looking at other similar contexts (since gorilla and lemur have similar meaning, they will appear in similar contexts).\n",
    "\n",
    "(3)*&ensp; to Goodison and hearing those <font color=red>animals</font> doing &ensp;&ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; impressions than anything else Ive ever heard.  \n",
    "&ensp;&ensp;&ensp; Important all-year-round foods of the lowland &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; in Gabon include the large ground-living  \n",
    "&ensp;&ensp;&ensp; of the feeding behaviour of the lowland &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; and its interaction with other fruit-eating  \n",
    "&ensp;&ensp;&ensp; the near relatives of man, the <font color=red>chimpanzee</font> and &ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; . In the case of these <font color=red>species</font>, rigidly  \n",
    "&ensp;&ensp;&ensp; evolutionary  ties with thee <font color=red>chimpanzee</font> and &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; , but also his rather less obvious resemblance  \n",
    "&ensp;&ensp;&ensp; relative, the <font color=red>chimpanzee</font>, and even more in the &ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; . But man is, in general, sexually dimorphic in  \n",
    "&ensp;&ensp;&ensp; are  quite unlike anything found in the &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **gorilla** &ensp;&ensp;&ensp;&ensp; or <font color=red>chimpanzee</font>. They are, however, closely*  \n",
    "<br>\n",
    "&ensp;&ensp; In fact, in computational semantics, we can build distributional semantic models to calculate similarity between certain words by comparing the contexts they appear in (Jurafsky and Martin, 2017). The idea is that the more contexts two words share, the more similar they are. In DSMs, contexts are represented as vectors of co-occurrence counts denoting how often words co-occur with one another (ibid.). Their similarity is then calculated by comparing these vectors: if two words have similar context vectors, then they have similar meaning (Basili and Pennacchiotti, 2010; Jurafsky and Martin, 2017). These are high dimensional, sparse vectors (containing many zeros) because most of the words will just not happen to appear in the context of other words. This vector representation of a word meaning is called embedding, while the models trying to use words represented in this way are called *vector space models, word space models*,  or *vector based models* (Van der Plas, 2017; Jurafsky and Martin, 2017).  \n",
    "&ensp;&ensp; In order to calculate the similarity between terms, we can consider the following example.\n",
    "\n",
    "|          | get  |  see | use  | hear |\n",
    "|----------|------|------|------|------|\n",
    "| lemur    |  5   |  35  |  2   |  22  |\n",
    "| gorilla  |  9   |  65  |  3   |  36  |\n",
    "| cat      |  35  |  74  |  5   |  40  |\n",
    "| chair    |  81  |  35  | 104  |   0  | <br>\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Table 1. Term-context matrix.</h6>    \n",
    "\n",
    "In the table we see a hypothetical example of co-occurrence counts for the target words (raws) and their context words (columns). In order to calculate the similarity between the target words given that they appear in the context of the words *get* and *see*, we present them in a n-dimensional (Euclidean) space (Evert, et al., 2010). The position of the words is determined by their co-occurrence counts with the words *see* and *get*: *lemur = (10, 76)*.   \n",
    "<img src=\"pics/example4.jpg\" width=\"550\" height=\"550\" />\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Figure 3. Target words as points (left) or vectors (right) in two-dimensional space.</h6>    \n",
    "\n",
    "There are several ways of counting similarity between the words now: by either counting the Euclidean distance between the points (spatial proximity), or by measuring the angle between the vectors (ibid.). Measuring the angle is a better option because it takes into account vector’s direction rather than position of the word. For example, if we take a look at the distance of the points in the figure 1 (left), *cat* and *gorilla* will seem more similar than *gorilla* and *lemur*. However, if we look at the angle between the vectors on the right on the figure 1, we will notice that it is very small between *gorilla* and *lemur*, showing their very close similarity, as compared to the angle between *gorilla* and *cat*, or in that matter *chair*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Because DSMs generate ranked lists of semantically similar words, they can be seen as alternatives to manually built lexical resources such as WordNet. (5 points)\n",
    "**- Explain the main differences between the lexical semantic content of WordNet and the lexical output of DSMs.  **  \n",
    "WordNet is a type of a thesaurus in which not only can we find words’ definitions and their synonyms, but also other words they are related to by a certain semantic relation (synonymy, hyponymy, polysemy, metonymy, meronymy / holonymy, antonymy) (Miller et al, 1990). It contains open class words (nouns, adjectives, verbs and adverbs) which are organized by their senses and grouped into synonym sets (synsets). Besides, all words are hierarchically structured in order to represent the semantic relations between them (ibid.).  \n",
    "&ensp;&ensp; Compared to the DSMs, when using WordNet it is relatively easy to extract semantic relations between a target word and context words since that information is explicitly represented in its structure. On the other hand, when it comes to the output of a DSMs for a specific target word, we get only a sorted list of most semantically related words together with their scores, but without any information about what kind of relation exists between the target word and the outputed words.  \n",
    "<br>\n",
    "**- Discuss advantages and drawbacks of manually built lexical resources such as Wordnet and automatically generated lexical resources such as lists of semantically similar words stemming from DSMs. **  \n",
    "When it comes to WordNet, one of its advantages is that it is hand-built by linguist and therefore very precise (Van der Plas, 2017). However, this kind of resource is expensive and time-consuming to create and can also be quite difficult to maintain and keep updating for new words. These are the reasons why it is difficult to have a similar resource for other languages (ibid). Furthermore, it is definitely an advantage to have explicit relations marked between different senses of words together with glosses explaining the senses. One of the improvements here could be to provide currently missing frequency information for the synsets. If we take a look at WordNet, we will see that a lot of senses following the first ones are quite infrequent in English language, but no explicit information is given about this (ibid.). Also, when it comes to relations between the synsets, most of them are represented as binary, which is not the case for synonyms for example; there is a higher or lower level of similarity / relatedness that is not quantified in WordNet (ibid.). Here, words are either synonyms or not. This, however, can be overcome using a thesaurus-based algorithms to calculate words similarity given the hierarchical structure of WordNet, in which case the similarity is calculated based on the distance of two words in the hierarchy (Jurafsky and Martin, 2017).  \n",
    "&ensp;&ensp; On the other hand, DSMs have their pros and cons as well. These models are fast and inexpensive to obtain as opposed to WordNet creation, and therefore can be applied to any domain and any language (Van der Plas, 2017). Also, they provides a way to quantify semantic relatedness between words (ibid.). However, the significant disadvantage DSMs have compared to WordNet is that besides the similarity scores we get in the output, we have no further information about the relations between observed words. We do not get only synonyms in the outputs, but all other sorts of semantic relations. Similarity in this approach is taken quite broad and has as its disadvantage not being able to distinguish between different relations, such as  synonyms, antonyms, etc. (Sahlgren, 2001).   \n",
    "<br>\n",
    "**- Can you think of an application that would fare better when using WordNet and another thatwould benefit more from the output of DSMs?**  \n",
    "WordNet is commonly used for the task of word sense disambiguation, where we need to find the most appropriate sense of a word given the context it appears in (Morato et al., 2004).  \n",
    "&ensp;&ensp; DSMs could be used in many applications, such as in information retrieval, where we can retrieve documents containing similar words to the ones in a query. Also, it could be used in machine translation, where DSMs could help us to find out if two words are similar enough to be used in the same context, etc. (Jurafsky and Martin, 2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "#### In this assignment, in order to gather semantically similar words, we will use the following:\n",
    "**1. Data:**\n",
    "- A monolingual corpus: **English**\n",
    "- Word-aligned parallel texts for a language pair **English-Spanish**\n",
    "\n",
    "**2. Tools:**\n",
    "- an extraction program (for the extraction from the parallel text): a program that gathers coocurrence counts for the target words\n",
    "- a DSM tool, a program that will compute the similarity between the coocurrence vectors for the head words we are interested in – taking the coocurrence matrix as input\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a coocurrence matrix from a corpus:\n",
    "There are several ways to build a coocurrence matrix depending on the target words you are selecting (parameter 1) and the type of features, more in particular, the type of context you are selecting (parameter 2). Please refer back to the slides of the lecture on DSMs for a list of parameters and a description of the options.\n",
    "\n",
    "#### *TARGET WORDS*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Does the monolingual corpus you are using to extract the cooccurrence counts from provide lemmas? Discuss why using lemmas instead of words often leads to better performances in DSMs. (3 points)\n",
    "The monolingual corpus we are using does not provide lemmas. However, the lemmatization should be applied since it is a type of text normalization that will help us reduce dimensionality, that is, avoid data sparseness (Turney and Pantel, 2010). Here  we are interested in word similarity, and therefore we will want to convert all the different grammatical forms of a certain word into one (lemma), since they all carry essentially the same meaning. In this way, we will avoid counting separately in the co-occurrence matrix different forms of the same word. Lemmatization normally leads to a better performance because in this way we are eliminating redundant grammatical information, while making it easier to recognize semantic similarities among words (ibid.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n",
    "\n",
    "#### 4) Extra points: The parallel text does not provide lemma information, nor PoS tags. Run a PoS tagger on (both sides of) the parallel text and use the information it gives you for the DSM. (+5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data provided with DISSECT already selected target words for you. For the parallel text, and in case you selected your own monolingual text, we take the 1550 most frequent content words (lemmas) in the corpus as target words. In order to determine the 1550 most frequent content words in either corpus, you will need to count all content words. (If you do not have access to PoS information, use a list of stopwords that you filter out.) You then need to rank these words according to their frequency and take the top 1550.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Explain why PoS information can be useful for distinguishing between ambiguous terms. Give some examples from the data you are using. (3 points)\n",
    "POS information can be useful because it helps us disambiguate between formally identical words with different meanings (Turney and Pantel, 2010). This does not refer to homonyms, but rather words that have the same form only in certain contexts and are distinguishable by their part-of-speech. For example, the word *work* (noun,verb) without POS information could be translated to Spanish as *trabajo* (noun) or *trabajar* (verb). If we apply a POS-tagger on our data, we avoid having this type of issue.  \n",
    "&ensp;&ensp; Since we are working with a parallel corpus, not distinguishing between different POS can mislead us into extracting wrong types of translations into our co-occurrence matrix. The following examples are from the Europarl English-Spanish parallel corpus that we are using. The Spanish word *poco* can have different translations into English depending on its part-of-speech (example 4).\n",
    "\n",
    "(4) &ensp; *(Spanish: determiner):&ensp;&ensp;\ttienen dificultades con esta cuestión a dar esos **pocos** pasos más*  \n",
    "&ensp;&ensp;&ensp;&ensp;  *(English: adjective):&ensp;&ensp;&ensp;&ensp; who have difficulties in this matter to take those **few** extra steps*  \n",
    "&ensp;&ensp;&ensp;&ensp;  *(Spanish: adverb):&ensp;&ensp;&ensp;&ensp;&ensp; Esto me molestó un **poco** porque*   \n",
    "&ensp;&ensp;&ensp;&ensp;  *(English: adverb):&ensp;&ensp;&ensp;&ensp;&ensp; I was **somewhat** irritated about this* (informally could be also translated as **a bit, a little**)  \n",
    "<br>\n",
    "In the opposite case, we can have English word *well* differently translated depending on it POS information (example 5).  \n",
    "\n",
    "(5) *&ensp; (English: adverb):&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; this work has been carried out extremely **well**  \n",
    "&ensp;&ensp;&ensp;&ensp;  (Spanish: adverb):&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; Creo que es un trabajo muy **bien** realizado  \n",
    "&ensp;&ensp;&ensp;&ensp;  (English: interjection):&ensp;&ensp;&ensp; **Well**, we in Britain are looking to the European Court  \n",
    "&ensp;&ensp;&ensp;&ensp;  (Spanish: interjection):&ensp;&ensp;&ensp; **En fin** , en Gran Bretaña hemos recurrido   \n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;**Bien**, es verdad que se redactan regulaciones* (informally, it could also be also translated as **vaya, hala**)  \n",
    "*&ensp;&ensp;&ensp;&ensp; (English: noun):&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; is like carrying water to fill up a dry **well**  \n",
    "&ensp;&ensp;&ensp;&ensp; (Spanish: noun)&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;es como si llevásemos agua al **pozo**  *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "#### *FEATURES*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Please discuss in your report, the different types of features, more in particular, the types of context (parameter 2) that are employed for DSMs in general. (5 points)\n",
    "As previously shown, DSMs are based on a co-occurrence matrix. There are several types of co-occurrence matrices that we could build, such as term-document matrix, term-context matrix and pair-pattern matrix (Turney and Pantel, 2010; Jurafsky and Martin, 2017).  \n",
    "**1. Term-document matrix.** In this kind of matrix rows represent terms (words of the vocabulary), while columns represent documents (they can be actual documents of some kind or just a bigger collection of text like web pages) (Jurafsky and Martin, 2017). Each cell of the matrix contains the number of times each word appears in each document. In this way, each document is represented as a count vector represented as a point in a n-dimensional space. Two documents are similar if they contain similar words, and therefore have similar vectors (ibid.). And as seen earlier, we can measure the similarity between documents either by measuring the distance between points or by measuring the angle between the vectors. The smaller the angle/ distance, the more similar two documents are. In other words, more similar words they contain, the more similar they are. \n",
    "\n",
    "|          |  doc1  |  doc2  |  doc3  |     \n",
    "|----------|--------|--------|--------|    \n",
    "| lemur    |   0    |   15   |    1   |\n",
    "| gorilla  |   0    |   21   |    2   |\n",
    "| cat      |   6    |    4   |   17   |\n",
    "| chair    |   12   |    0   |    0   |\n",
    "\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Table 2. Term-document matrix.</h6>    \n",
    "<br>\n",
    "**2. Term-context matrix** (also called word-word or term-term matrix). When it comes to term-context matrix, the rows of a matrix are again target words (terms), while in the columns we have a narrower context than previously used document, but ‘context’ can refer to anything from a whole vocabulary, certain words, paragraphs, sentences, to specific syntactic relations among words. The following summarizes context possibilities (Evert et al., 2010; Van der Plas, 2017):  \n",
    "** - Surface context: **  \n",
    "This refers to the “window” around the target word, which basically consists of n words or characters to its left and n words or characters to its right (ibid.). In this case the cells in the matrix would contain counts of how many times the word in the column appeared in the n- word window around the row (target) word. For example, if we have the 7-word window around the words *gorilla, lemur, cat and chair*, we get the following example sentences:  \n",
    "\n",
    "(2)*&ensp; with the consequent loss of habitat for &ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **gorillas** &ensp;&ensp;, elephants and other threatened animal  \n",
    "&ensp;&ensp;&ensp;&ensp; that there is a home for owls and squirrels, &ensp;&ensp; **lemurs** &ensp;&ensp;, parrots and toucans. A river with slightly  \n",
    "&ensp;&ensp;&ensp;&ensp; outback have been set back by domestic &ensp;&ensp;&ensp; **cats** &ensp;&ensp;&ensp;&ensp; , which have killed 11 of the rare animals  \n",
    "&ensp;&ensp;&ensp;&ensp; Kendal would you like to bring your &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; **chair** &ensp;&ensp;&ensp;&ensp; over. Rachael can you see the board from where*  \n",
    "\n",
    "And then make a matrix the four target words in columns and example column words from the context of 7-word window (there are many more words that could be added to the matrix, but this is just and example):\n",
    "\n",
    "|           | ... |  habitat  |  elephants | animal | squirrels| board | ... |\n",
    "|-----------|-----|-----------|------------|--------|----------|-------|-----|\n",
    "| lemurs    | ... |     0     |      0     |   0    |     1    |   0   | ... |\n",
    "| gorillas  | ... |     1     |      1     |   1    |     0    |   0   | ... |\n",
    "| cats      | ... |     0     |      0     |   1    |     0    |   0   | ... |\n",
    "| chair     | ... |     0     |      0     |   0    |     0    |   1   | ... |\n",
    "\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Table 3. Term-context matrix.</h6>    \n",
    "The size of the window can be as small or as big as we need. Depending on what kind of relations we want to extract, we will need one or the other. More about how the window size can influence the relations we can extract from corpus is in the question 7. Furthermore, we can choose if we want to get both the left and the right context, or only one of them, depending if we are interested in the words that follow or recede the target word. For example, if we wanted to see what kind of adjectives normally describe a lemma *woman*, then we would chose to see only the left context. And finally, we can chose to weight words that are closer or further from thee target word in context, again depending on what kind of information we want to extract; we can have either uniform or distance-based weightening (Evert et al., 2010).  \n",
    "** - Textual context:**   \n",
    "This type of context refers to the fact that the context terms belong to the same linguistic unit as the target term (Evert et al., 2010). Commonly these linguistic units are sentences, paragraphs, but can also be turns in a conversation or texts on web pages (Evert et al., 2010; Van der Plas, 2017). This type of context resolves somewhat the problem surface context has - we need to choose an particular number that will not always fulfill our expectations: it can be too big and go over sentence or paragraph boundaries, or too small and not catch the context we want (for example, catching a context of *throw* in a 3-word window will catch *throw a birthday party*, but not *throw a big birthday party*) (Evert, 2008). This can be especially problematic when working with a language that has a more free word order and where closely related words would happen to appear far apart in the context. In those cases, this kind of context would be more appropriate to use.  \n",
    "**- Syntactic context:**  \n",
    "This type of context includes co-occurrence counts of target words and the words that have a specific syntactic relation with the target words (Evert et al., 2010; Van der Plas, 2017). We can have different types of relations, such as subject, object, modifier, coordination, apposition, etc. (ibid.).  \n",
    "\n",
    "|          |  eat-subj |  paint-obj |  long-tailed-adj | have-obj |   \n",
    "|----------|-----------|------------|------------------|----------|  \n",
    "| lemur    |     32    |      0     |         43       |    16    |\n",
    "| gorilla  |     35    |      0     |          9       |    17    |\n",
    "| cat      |     30    |      0     |          5       |    25    |\n",
    "| chair    |      0    |     20     |          0       |    24    |\n",
    "\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Table 4. Term-context matrix: syntactic context.</h6>    \n",
    "In this way, the columns of the matrix are syntactic relations with specific words  we are interested in, while the cells contain how many times the target and the context word appeared in the particular syntactic context. As compared to the surface context, it does not set a strict word limit for a context but it manages to catch relations happening further away from the target word in the context; while compared to the textual, it introduces less noise (Evert, 2008).  \n",
    "**- Multilingual or translational context**  \n",
    "When it comes to multilingual context, it contains as columns target word translated to other languages, while cells contain how many times a certain target word was translated in a particular way in a particular language (Van der Plas, 2017). This kind of information we can extract from aligned parallel corpora. For example:  \n",
    "\n",
    "|          |  autunno-IT |  herfst-NL | \n",
    "|----------|-------------|------------|\n",
    "| fall     |       8     |     13     |\n",
    "| autumn   |      10     |      8     |  \n",
    "\n",
    "<h6 style=\"text-align: center;\" markdown=\"1\">Table 5. Term-context matrix: translational context.</h6>    \n",
    "Here we can see how many times the formal variations with the same meaning *autumn* and *fall* were translated as such in Italian and Dutch. Besides looking at different types of translations, in this context we can also capture how a certain phrase or a word is expressed in different languages.\n",
    "\n",
    "\n",
    "**3. Pair-pattern matrix.** The pair-pattern matrix is used to measure similarity between different kind of patterns (Turney and Pantel, 2010). In this matrix the columns represent the patterns, for example *X solves Y*, while the cells contain the counts of how many times target words appear as a part of this pattern. As stated by Lin and Pantel (2001) (in Turney and Pantel, 2010), pairs of target words that co-occur with similar patterns, tend to have similar meaning. The idea is that given the pattern X solves Y, we will find similar patterns such as “Y is solved by X”, “Y is resolved in X”, and “X resolves Y ” (Turney and Pantel, 2010). That is, these patterns tend to appear together with similar X and Y word pairs, which points to their similar meaning. For example, pairs *mason : stone, carpenter :wood* appear with similar semantic relation *artisan :material*  (Turney and Pantel, 2010).  \n",
    "<br>\n",
    "In summary,  the more structured and restricted the context, the more closely related are the words the system finds. Synonymy is a very tight semantic relation, co-hyponymy and direct hyponymy a bit less, indirect hyponymy even less. Mere association is the least semantically tight relation. (Van der Plas, 2017)\n",
    "_______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the aligned parallel text, we are going to select the words that are aligned to the target word as features. Please select only the 10000 most frequent content words as features. If you opted to include your own monolingual corpus, we are going to select a small window of the content word following and the content word preceding the target word, as features. Please select only the 10000 most frequent content words as features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Discuss what influence the size of the window has on the nature of the semantic relations we find between target word and semantically similar words proposed by the system. What consequences do you think a larger window size would have on the semantically similar words proposed by the system? (3 points)\n",
    "Example from Evert et al. (2010), nearest neighbours for a word *dog* extracted from BNC with different window sizes:  \n",
    "*(3)*  \n",
    "&ensp;&ensp;&ensp; 2-word window: *cat,  horse, fox, pet, rabbit, pig, animal, mongrel, sheep, pigeon*  \n",
    "&ensp;&ensp;&ensp; 30-word window: *kennel, puppy, pet, bitch, terrier, rottweiler, canine, cat, to bark, Alsatian*  \n",
    "<br>\n",
    "In general, the shorter the window, the more syntactic information we catch since we are focusing only on the nearest neighbours of the target word, while on the other hand, longer window catches more semantic relations between the words (Jurafsky and Martin, 2017). In this sense, we have two kinds of similarity as stated by Jurafsky and Martin (2017). Firstly, we say that words have first-order co-occurrence (syntagmatic association) when they are normally found near each other. For example, that is the case with words *book* and *poem* with the verb *write*; they indeed normally appear close to one another and *book* and *poem* are normally encountered as objects of the verb *write*. On the other hand, we have a second-order co-occurrence (paradigmatic association) if the words have similar neighbours (Jurafsky and Martin, 2017). For example, this is the case with the word *wrote* and words *said*, *told*, etc.  \n",
    "&ensp;&ensp; When it comes to the type of a semantic relation depending on the window size, when we have a shorter window, we catch similar words to the target in the sense that it can appear in the similar syntactic position. In the example given below, we get all sort of names for animals when we look for the similar words for *dog*: *cat, horse, pidgeon*, etc. With the wider window, we catch words like *dog* that are likely to appear in a similar text, in the text of a similar topic: *kennel, puppy, pet, bitch*. In the wider context, we get synonyms for example, while in the narrower context we get related words, hyponyms, hypernyms, ..  \n",
    "&ensp;&ensp; The more restricted the context, the more closely related are the words the system finds. \n",
    "The larger window allows us to capture a bit further semantic relations. \n",
    "the wider the context the bigger chance is for us to find synonyms since they will not normally occur one after another  \n",
    "\n",
    "Presentation:\n",
    "The more structured and restricted the context, the more closely related are the words the system finds. Synonymy is a very tight semantic relation, co-hyponymy and direct hyponymy a bit less, indirect hyponymy even less. Mere association is the least semantically tight relation.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Discuss the differences in output of the DSM (in terms of the similar words it will produce) you expect between the two types of input you are using (the monolingual corpus and the aligned parallel files). Which type of data do you think will generate the highest percentage of synonyms amongst the nearest neighbours? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "#### 9) Write a python program that extracts coocurrence counts for all target words and features from the parallel aligned files (and the monolingual corpus, if you are using a different corpus than the one provided by DISSECT).  Include the extraction program you wrote in the submission, and paste 20 lines (for 20 target words) of the coocurrence counts you extracted in the report. The output of your program should be formatted according to the input format the DSM tool you are using requires. (36 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "#### *Feature weighting and similarity function:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Explain what function the feature weighting scheme has. What would be the drawback of doing without a weighting scheme and using raw frequencies instead? (3 points)\n",
    "When working with language and having a big collection of words as features, we commonly end up with having a very noisy feature representation, which means that we have a lot of features that are redundant and not very informative for the task we are trying to conduct. The purpose of using a measure of feature weighting is to reassign new counts to the co-occurrence matrix, but this time taking into account how informative they are for our task. For example, if we just take raw counts into consideration, it will turn out that frequent words, such as *the, a it, about, on, for*, etc. (stop words), appear very frequently in a lot of contexts and therefore mislead us into thinking that they are similar with a lot of different kind of words just because they appear very frequently. So the raw counts are not the best measure of similarity between words (Jurafsky and Martin, 2017; Van der Plas, 2017) and can many times mislead us. Also, for example, if we are trying to figure out what kind of context is shared between *gorilla* and *lemur* but not by *chair* and *table* we will not get a lot of information from the stop words mentioned earlier, that will appear in a lot of contexts and with many different words and therefore cannot tell us anything in particular about any target word. Since this is the case and we would like to extract the most informative context words, we need to measure the informativeness of words we want to use.  \n",
    "&ensp;&ensp; There are several weightnening measures we can use, for example Pointwise mutual information or log-likelihood ratio. These are association measures of “how much more often  than chance two words co-occur” (Jurafsky and Martin, 2017), that is it weights the association between the target word and the features by comparing the expected and the observed co-occurrence frequency (Van der Plas, 2017). “The less frequent the target word and the context feature are, the higher the weight given to their observed co-occurrence count should be (because their expected chance co-occurrence frequency is low)” (Evert et al., 2010).  \n",
    "&ensp;&ensp; Another very known feature weighting method is tf-idf. It is not much used when measuring semantic similarity, but it is used in many other applications, especially information retrieval where it is a dominant method for weighting frequencies. (Jurafsky and Martin, 2010). What it does is that it give less weight to the features that appear in a lot of different documents, because that means they are not very informative for distinguishing between different types of documents. Fro the words that appear not as often and in only certain type of document, more weight is given since it shows that they are a good indicator of that document type (ibid.). So more weight is given to a term frequent in a certain document type, but rare in general in other documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11) List the weighting schemes the tool you are using includes and what similarity functions. Make your choice for the weighting scheme and similarity function you will use and motivate your choice in the report. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "#### *Analysing the output:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we are ready to generate some output. You should be able to get the distributional similar words of any of the 1550 target words you selected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12) For both types of corpora (multilingual and monolingual), retrieve the top-5 semantically similar words for five high-frequency words and five low-frequency words from the 1550 words. Paste the output in the report. \n",
    "- Is there a difference in the quality of the output for the high-frequency versus the low-frequency words?\n",
    "- Is there a difference for the two types of contexts used? Can you explain what you see? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13) Explain the notion of contextual variability.\n",
    "Take a polysemous word of your choice that is among the 1550 head words you selected. Generate the top-10 semantically similar words for this target word using the monolingual DSM you created. Paste the output in the report.  \n",
    "**- Do you see some consequences of contextual variability in the outputs for this word?**  \n",
    "Now take the multilingual DSM. Find a polysemous word in the features of the multilingual DSM1. For its translation in your target language, find the top-10.  \n",
    "**- What are your observations?**  \n",
    "**- Do you think using multiple languages could have improved the multilingual DSM? (5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "#### *Compositionality:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14) Explain the principle of compositionality and discuss a couple of different ways in which DSMs have been adapted to cater for compositionality. (3 points)\n",
    "Most of the work done in Distributional semantics takes into account only single, isolated words (Guevara, 2011). However, in language we combine words into bigger structures, phrases and sentences, such as *strong coffee, table cloth, hot dog*, etc. (Van der Plas, 2017). Therefore, we would want DSMs to be able to deal with these more complex structures as well. This is where the compositionality theory comes in place, which is the fundamental idea in most of the contemporary works in semantics (Van der Plas, 2017). It is based on the Principle of compositionality: the meaning of a complex expression is comprised of the meaning of its parts and by the syntactic rules that combine them (Partee, ter Meulen and Wall, 1990: 318 in Guevara, 2011).  \n",
    "&ensp;&ensp; So the question rises how to deal with compositionality in DSMs. This is not an easy task and if we take a look at the compounds as a quite good example of how problematic compositionality is, we will see that there are several levels of compositionality (Van der Plas, 2017). Some compounds can have somewhat  compositional meaning, as in *table cloth*, others can have it partially compositional as *data base*, while some will have a meaning that cannot be split into meaningful parts as in *hot dog*. Besides, they could consist of combinations of different parts of speech ADJ-N, N-N, V-N, etc. (ibid.).  \n",
    "? Verbs + Prepositions: I e.g. the difference between to stand on and to stand up I Phrasal compositionality: I e.g. red car  \n",
    "&ensp;&ensp; Previous research has had several ideas to tackle the problem of compositionality using DSMs models. Firstly, the attempt was made to treat complex expressions as a whole unit, since some of them already are merged together as for example database or compounds in general in German language. Following the Distributional hypothesis, we could then represent the meaning of a compound by looking at all the contexts it appears in. However, the problem with this approach is that it works only when looking into a really large corpus of text and with only very frequent compounds, so data sparsity would be an issue since the rare compounds would be quite bad representations (Van der Plas, 2017). Besides, even with the frequent compounds there an issue of be able to “generalize or reuse word meaning in a transparent use” (ibid.).  \n",
    "&ensp;&ensp; Given all the issues previously stated, the work done after this treat compositionality as a function, which means that given to independent vectors, the semantically compositional resulting vector is created by one of the following operations: vector addition, (pointwise) vector multiplication or tensor product (Guevara, 2011; Van der Plas, 2017).  Vector addition is one of the most common approaches in the literature introduced by Mitchell and Lapata (2010) (as stated in Van der Plas, 2017)) and it involves a simple addition as follows:  \n",
    "\n",
    "(3)  summation: v1 + v2 = v3  \n",
    "&ensp;&ensp; red \\[1 2 0\\] +  house  \\[14 16 20\\] =  red house \\[15 18 20\\]  \n",
    "<br>\n",
    "&ensp;&ensp; multiplication: v1 x v2 = v3 \n",
    "&ensp;&ensp; red \\[1 2 0\\] x  house  \\[14 16 20\\] = red house \\[14 32 20\\]  \n",
    "<br>\n",
    "&ensp;&ensp; tensor product: v1⊗v2 = v3  \n",
    "&ensp;&ensp; red = \\[1 2 0\\] ⊗ house = \\[0 1 3\\] = red house \\[1*0, 1*1, 1*3, 2*0, 2*1, 2*3, 0*0, 0*1, 0*3\\] =  red house \\[0, 1, 3, 0, 2, 6, 0, 0, 0\\]   \n",
    "<br>\n",
    "So given two vectors v1 and v2, we get a compositional meaning of v3 by summing the previous two up. Summing vectors is computationally efficient, however it does not provide us with an ideal solution (Van der Plas, 2017): the new vector is in between the two, but is more governed by the one that contains higher individual frequency counts. In the example of the *red house*, the new vector is more influenced by the vector *house* than the vector *red*. Therefore, it resembles more a union than an intersection (ibid.).  \n",
    "&ensp;&ensp; Mitchell and Lapata (2010) introduced also pointwise-multiplication of vectors. It differs from the addition model by only taking into account non-zero components of the vectors while the summation considers all of the components. The intuition is similar, we multiply two vectors and get as an output a third one that should correspond to the complex expression.  \n",
    "&ensp;&ensp; Widdows (2008) (in Guevara, 2011) introduces a more complex vector operation, tensor product in order to capture meaning compositionality. The result of a tensor product of two vectors is a high dimensional matrix, which was shown empirically to catch better the compositionality of meaning (Van der Plas, 2017).  \n",
    "&ensp;&ensp; All of the previously mentioned approaches have one thing in common: they all apply one single geometric operation on the vectors v1 and v2; it seems unlikely that only one operation can work on all semantic and syntactic relations and in all languages (Guevara, 2011).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15) Now, let us experiment with compositionality in the monolingual DSM you have just built. Follow the instructions on http://clic.cimec.unitn.it/composes/toolkit/composing.html and select a model of your choice (motivate your choice). Show some example output of the composed DSM. Analyse the results. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16) Is it possible to create a compositional model for the multilingual DSM with the models you have chosen? If so, show some example output here as well. Analyse the results. (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "#### *Relating things back to theory:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We discussed several theories in class that try to explain the lexical semantics of words. We are now trying to couple our knowledge from one of these theories to the things we observe in the DSM. 1 For example, if English is your target language and you chose German as the counterpart in the parallel text, you could choose Kiefer in German (see slides). One of its translations in English is pine. Now, get the top-10 most similar words of pine in the multilingual DSM and describe if you see any unexpected nearest neighbours. You can try with several polysemous words, if you like.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 17) Discuss the basic tenet of the Prototype Theory in lexical semantics and discuss at least two advantages when compared to the classical theory. (5 points)\n",
    "The Prototype Theory emerged as a reaction to the problem with psychological data in the Classical theory providing an alternative approach (Laurence and Margolis, 1999). Even though there is not one single prototype theory all theorists agree about, the general idea is the same: “most concepts - including most lexical concepts - are complex representations whose structure encodes a statistical analysis of the properties their members tend to have” (ibid.). In other words, most items in the extension of a concept will have these kind of features “typical” of a concept, however there will be some items that will not be represented by all of the “typical” features and yet still belong to the same concept. This points to the fact that not all features are by default necessary and that some of them are more important than others (Van der Plas, 2017). This is different than what the Classical theory suggests - that all features are necessary and that an item has to satisfy all of concept’s features in order to belong under a certain concept (Laurence and Margolis, 1999). On the contrary, the Prototype theory suggests satisfying a sufficient number of features out of which some carry more importance than others (ibid.). For example, if *bird* is composed by features such as *flies, sings, has feathers, lays eggs, nests in trees*, etc. then we could say that a *sparrow* is a bird because it has all of the features. The problem in the Classical theory would come up with birds such as *ostrich* since it does not have all the features therefore would not be considered a bird. The Prototype theory takes care of these kind of examples because even though *ostrich* does not have all the features, it has enough to be considered a bird -  they do not fly, but they have feathers, lay eggs, etc. An item that satisfies all the features, or most of them, could be considered a prototypical example of a concept.  \n",
    "&ensp;&ensp; The previously stated rejection of the idea about “necessary and sufficient conditions” goes along with Wittgenstein's idea that things that fall under the same concept form \"a complicated network of similarities overlapping and crisscrossing: sometimes overall similarities, sometimes similarities of detail\" (Laurence and Margolis, 1999), in other words they have a “family resemblance”. Rosch and Mervis (1975, as stated in Laurence and Margolis, 1999) explain this further: “formal criteria are neither a logical nor psychological necessity; the categorical relationship in categories which do not appear to possess criterial attributes \\[...\\] can be understood in terms of the principle of family resemblance.” In other words, they all state that concepts are not governed by definitions but by a unfixed set of properties that will appear in different combinations with different items under a certain concept. The example they give is *game*; some games will one set of properties, some will have other, but no matter what exact set of properties they have, what makes them similar and to fall under the same concept is that their properties overlap in a way that creates a similarity space (Laurence and Margolis, 1999). So what makes a certain game fall under the concept *game* is that it is within boundaries of this similarity space (ibid.).  \n",
    "&ensp;&ensp; Since the Prototype theory was created mostly as a response to the classical theory, it solves some of its problems one of which was mentioned before about family resemblance and no need for a definition of concepts. This lack of definitions is the problem the Classical theory had that the Prototype theory successfully tackled by claiming and proving that concepts do not have a definitional structure (Laurence and Margolis, 1999). Second advantage the Prototype theory has over the Classical theory is ts treatment of categorization, which tackles the issues the latter theory had: conceptual fuzziness and typicality effect (ibid.). Conceptual fuzziness refers to the problem of many concepts being inexact (as *carpet* belonging or not to the concept of *furniture*), while the typicality effects refer to the problems where we have items that are not typical examples of a concept and therefore do not have all the features it could have under a certain concept (moer typical fruit: *apple* or *olive*) (Van der Plas, 2017). The Prototype theory puts into a certain category an instance if the representation of that instance is sufficiently similar to the representation of the category (Laurence and Margolis, 1999). In other words, it compares an instance with a prototypical example of the category and if there is enough feature overlap, then they are similar and belong to the same category. They have developed different types of measures of similarity: they compare all the features in parallel and depending on feature, it can add either a positive or a negative value to the “calculation” depending if it is a common feature or not. If the result obtained is not high enough to make a decision about whether an instance belongs or not to a certain category, they just do not make a judgement. And in their idea, this solves the fuzziness problem, they just do not make any positive or negative judgment if there is not enough evidence and are ok with the fact that some instances can be somewhere in between (Laurence and Margolis, 1999). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18) What are typicality effects? Does the DSM cater for typicality effects? (5 points)\n",
    "The typicality effect refers to the fact that not all the members of a certain category are equally typical examples of that category (Laurence and Margolis, 1999; Connell and Ramscar, 2001). As stated in Laurence and Margolis (1999), this was first proven by Rosch (1973) who found that the participants in the study were able to rank items as more or less typical members of a certain category. The example given to the participants was to sort different types of fruits (*apple, fig, plum, olive*) on the 7-point scale by how well they represent the category *fruit*, which showed that certain fruits are commonly addressed as typical (*apple*), while others were marked as atypical (*olive*) (ibid.).  \n",
    "&ensp;&ensp; If we take a look at the distributional similarity of these fruits, we get the following output:   \n",
    "\n",
    "|       apple      |       plum       |      olive        |      fruit      |\n",
    "|:-----------------|:-----------------|:------------------|:----------------|  \n",
    "| strawberry 0.808 | cherry 0.806     | olive 0.819       | berry 0.83      |\n",
    "| pear 0.793       | pear 0.770       | almond 0.69       | mango 0.71      |\n",
    "| cherry 0.780     | apricot 0.769    | olive 0.663       | vegetable 0.71  |\n",
    "| peach 0.760      | apple 0.746      | carob 0.652       | drupe 0.70      |\n",
    "| plum 0.746       | strawberry 0.739 | apricot 0.650     | flower 0.69     |\n",
    "| apricot 0.744    | peach 0.736      | pistachio 0.631   | apple 0.69      |\n",
    "| almond 0.740     | quince 0.735     | citrus 0.628      | strawberry 0.68 | \n",
    "| quince 0.707     | persimmon 0.734  | avocado 0.627     | pear 0.68       |\n",
    "| watermelon 0.705 | raspberry 0.705  | cinnamon 0.623    | apricot 0.68    |\n",
    "| mango 0.694      | blackberry 0.699 | pomegranate 0.616 | pome 0.68       |\n",
    "\n",
    "10 nearest neighbours\n",
    "<br>\n",
    "Checked here http://vectors.nlpl.eu/explore/embeddings/en/# (computed from Wikipedia (en))  \n",
    "\n",
    "&ensp;&ensp; We will see that for the more typical fruits chosen by participants (*apple, plum*) we get similar distributional results, while for the *olive* (atypical example), we get a completely different set of nouns (that we could argue share more properties with the olive “oily”). And also, we see that *apple* does appear as one of the typical neighbours of *fruit* while *olive* does not. Since the distributional similarity depends on the context these words appear in, we could say that the more typical examples of fruits tend to appear in different types of context than the non-typical ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "### Team work distribution by assignment questions:\n",
    "\n",
    "- **Angelo**:  \n",
    "1, 4, 5, 7, 11, 13, 14\n",
    "\n",
    "- **Jovana**:  \n",
    "2, 3, 6, 8, 10, 12, 17 \n",
    "\n",
    "- **Together:**  \n",
    "9, 18, compositionality / embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "### References\n",
    "\n",
    "- Basili, Roberto and Marco Pennacchiotti. (2010). Distributional lexical semantics: Toward uniform representation paradigms for advanced acquisition and processing tasks. In: Natural Language Engineering, 16 (4), pp. 347–358, 2010.  \n",
    "<br>\n",
    "- Connell, Louise and Michael Ramscar. (2001). Using Distributional Measures to Model Typicality in Categorization. *In Proceedings of the 23rd Annual Conference of the Cognitive Science Society*. University of Edinburgh.  \n",
    "<br>\n",
    "- Evert, Stefan (2008). Corpora and collocations. *In A. Lüdeling and M. Kytö (eds.), Corpus Linguistics. An International Handbook*, chapter 58. Mouton de Gruyter, Berlin.  \n",
    "<br>\n",
    "- Evert, Stefan, Marco Baroni, and Alessandro Lenci. (2010). *Distributional Semantic Models*, Tutorial at NAACL-HLT 2010, Los Angeles, CA.  \n",
    "<br>\n",
    "- Guevara, Emiliano. (2011). Computing semantic compositionality in distributional semantics. *In Proceedings of the Ninth International Conference on Computational Semantics (IWCS '11)*. Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 135-144.  \n",
    "<br>\n",
    "- Harris, Zellig S. (1968). *Mathematical structures of language*. New York: Interscience Publishers.  \n",
    "<br>\n",
    "- Jurafsky, Daniel and James H. Martin. (2017). Speech and Language Processing. Draft from August 2017, available [here](https://web.stanford.edu/~jurafsky/slp3/15.pdf).  \n",
    "<br>\n",
    "- Laurence, Stephen and Eric Margolis. (1999). Concepts and Cognitive Science. *In Margolis, Eric and Stephen Laurence (eds.) Concepts: Core Readings*, pp. 3-81. Cambridge, Mass.: MIT Press.  \n",
    "<br>\n",
    "- Miller, George A., Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. (1990). Introduction to WordNet: an on-line lexical database. *International Journal of Lexicography* 3, (4), pp. 235-244, 1990.  \n",
    "<br>\n",
    "- Morato, Jorge, Miguel Ángel Marzal, Juan Lloréns, and José Moreiro. (2004). WordNet Applications. *In Proceedings of GWC*, pp. 20-23, 2004.  \n",
    "<br>\n",
    "- Sahlgren, Magnus. (2001). The distributional hypothesis. *Italian Journal of Linguistics* 20, pp. 33-53, 2001.  \n",
    "<br>\n",
    "- Turney, Peter D. and Patrick Pantel. (2010). From frequency to meaning: vector space models of semantics. *Journal of Artificial Intelligence Research* 37, (1), pp.141-188, 2010.  \n",
    "<br>\n",
    "- Van der Plas, Lonneke. (2017). *LIN2580/LIN5580: Computational Lexical Semantics, week 6 notes* \\[PowerPoint slides\\]. Retrieved from [here](https://www.um.edu.mt/vle/pluginfile.php/272004/mod_resource/content/5/CompLexSemLect6.pdf).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
