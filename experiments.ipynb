{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Computational Lexical Semantics</center></h1>\n",
    "<h5><center>LIN5580-SEM1-A-1718</center></h5>\n",
    "<h3><center>Jovana Urosevic and Angelo Basile</center></h3>\n",
    "<h5><center>January 31, 2018</center></h5>\n",
    "\n",
    "-----\n",
    "\n",
    "### Instruction on how to use this notebook to replicate the results:\n",
    "\n",
    "1. download and install python 2\n",
    "2. download and install [pipenv](https://docs.pipenv.org/)\n",
    "3. install cython separately: ```pipenv install cython```\n",
    "4. install the requirements from the Pipfile\n",
    "5. active the virtualenvironment: ```pipenv shell```\n",
    "6. ```cd dissect-master```\n",
    "7. install dissect: ```pipenv run python2 setup.py install```\n",
    "8. deactive the virtual environment:```exit```\n",
    "9. run the notebook: ```pipenv run jupyter notebook```\n",
    "\n",
    "#### Extra\n",
    "\n",
    "For the POS-tagging tasks, it is required to download language models.\n",
    "\n",
    "- ```pipen run python2 -m spacy download en``` for downloading the English model\n",
    "- ```pipen run python2 -m spacy download es``` for downloading the Spanish model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lillo/.local/share/virtualenvs/complexsem-mrnoETE5/lib/python2.7/site-packages/scipy/sparse/compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 1.0000000000000002), ('seemed', 0.8099054023790865), ('figured', 0.8078217152278638), ('hated', 0.7998818532597823), ('hip', 0.798432862050815)]\n",
      "[('was', 1.0), ('fails', 0.9583693827602454), ('feel', 0.9565436643990166), ('walk', 0.9545529508846556), ('makes', 0.9544440477548656)]\n",
      "[('movie', 0.9999999999999999), ('show', 0.9789815854478494), ('allowed', 0.9726192774757002), ('accent', 0.9722384866746526), ('spot', 0.9721894717725857)]\n",
      "[('film', 1.0), ('show', 0.9762035295747843), ('baby', 0.9733688913493664), ('rescue', 0.9728059173958485), ('knows', 0.9722895597600173)]\n",
      "[('is', 0.0), ('was', 0.0), ('movie', 0.0), ('film', 0.0), (\"'s\", 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# EXTRACTION PROGRAM\n",
    "# monolingual\n",
    "import spacy\n",
    "import codecs\n",
    "from __future__ import print_function\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# function to tag the text\n",
    "def tag(token):\n",
    "    word = token.text\n",
    "    tag = token.tag_\n",
    "    return word+'_'+tag\n",
    "\n",
    "# load the imbdb corpus\n",
    "import thinc.extra.datasets\n",
    "data, _ = thinc.extra.datasets.imdb()\n",
    "texts, _ = zip(*data[-10000:])\n",
    "\n",
    "# filter the text based on POS\n",
    "filtered = []\n",
    "for doc in texts:\n",
    "    filtered.append(' '.join(w.text for w in nlp(doc) if w.pos_ in ['NOUN', 'VERB']))\n",
    "\n",
    "# build the frequency distribution for words\n",
    "from collections import Counter\n",
    "word_freq = Counter()\n",
    "for t in filtered:\n",
    "    word_freq.update(t.split())\n",
    "\n",
    "# use only the 1550 most frequent words\n",
    "vocab = [x[0] for x in word_freq.most_common(1550)]\n",
    "assert len(vocab) == 1550\n",
    "\n",
    "#build the wordXword matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "docs = filtered\n",
    "count_model = CountVectorizer(vocabulary=vocab)\n",
    "X = count_model.fit_transform(docs)\n",
    "Xc = (X.T * X) \n",
    "Xc.setdiag(0)\n",
    "\n",
    "# invert the vocabulary\n",
    "di = dict([[v,k] for k,v in count_model.vocabulary_.items()])\n",
    "\n",
    "# save the output in a dissect compatible format\n",
    "with open('mono.dm',\"w\") as o:\n",
    "    for word, counts in zip(vocab, Xc.toarray()):\n",
    "        o.write(word.encode('utf-8')+' '+\" \".join(map(str, counts)))\n",
    "        o.write('\\n')\n",
    "\n",
    "with open('mono.rows', 'w') as o:\n",
    "    for w in vocab:\n",
    "        o.write(w.encode('utf-8'))\n",
    "        o.write('\\n')\n",
    "\n",
    "# note: this is the same as before, since we are using all the words as context\n",
    "with open('mono.cols', 'w') as o:\n",
    "    for w in vocab:\n",
    "        o.write(w.encode('utf-8'))\n",
    "        o.write('\\n')\n",
    "\n",
    "# BUILD THE DSM\n",
    "from composes.semantic_space.space import Space\n",
    "\n",
    "#create a space from co-occurrence counts in dense format\n",
    "mono = Space.build(data = \"./mono.dm\",\n",
    "                   rows = \"./mono.rows\",\n",
    "                   cols = \"./mono.cols\",\n",
    "                   format = \"dm\")\n",
    "\n",
    "# extract similar words\n",
    "from composes.similarity.cos import CosSimilarity\n",
    "# 5 most common\n",
    "for w in word_freq.most_common(5):\n",
    "    print(mono.get_neighbours(w[0], 5, CosSimilarity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('according', 0.9999999999999999),\n",
      " ('everything', 0.9931032867180424),\n",
      " ('place', 0.9928814512460278),\n",
      " ('says', 0.9926084092220174),\n",
      " ('keep', 0.9923692240019467)]\n",
      "[('serve', 1.0),\n",
      " ('example', 0.9884668620184874),\n",
      " ('appears', 0.9881421908147624),\n",
      " ('seems', 0.9881201223258333),\n",
      " ('nature', 0.9880742693529966)]\n",
      "[('breaking', 1.0),\n",
      " ('work', 0.9889140793499385),\n",
      " ('taken', 0.9885708775893547),\n",
      " ('today', 0.9884574294589742),\n",
      " ('number', 0.9882349708414532)]\n",
      "[('favorites', 1.0),\n",
      " ('watching', 0.9783481244248106),\n",
      " ('loved', 0.9778277195482216),\n",
      " ('opinion', 0.9771952444770546),\n",
      " ('enjoyed', 0.9770937336798221)]\n",
      "[('mistakes', 1.0),\n",
      " ('means', 0.9868802609693922),\n",
      " ('makes', 0.986564466448231),\n",
      " ('miss', 0.9861086700731426),\n",
      " ('kind', 0.9859963407857474)]\n"
     ]
    }
   ],
   "source": [
    "# 5 least common\n",
    "for w in word_freq.most_common(1500)[-5:]:\n",
    "    pprint.pprint(mono.get_neighbours(w[0], 5, CosSimilarity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', 1.0),\n",
       " ('everyone', 0.9964144093340506),\n",
       " ('mind', 0.9962880682030615),\n",
       " ('believe', 0.9962865266548062),\n",
       " ('come', 0.9962714101107594),\n",
       " ('give', 0.9962699881148831),\n",
       " ('reason', 0.9962526357871062),\n",
       " ('thing', 0.9962040639305845),\n",
       " ('look', 0.9961682074477383),\n",
       " ('things', 0.9960742493531183),\n",
       " ('end', 0.9960663558282294),\n",
       " ('take', 0.9960522935950892),\n",
       " ('anything', 0.9960490497511368),\n",
       " ('something', 0.9960261419004385),\n",
       " ('ca', 0.9959793968425902)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mono.get_neighbours('get', 15, CosSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a transformation\n",
    "# transformation\n",
    "from composes.transformation.scaling.ppmi_weighting import PpmiWeighting\n",
    "transformed = mono.apply(PpmiWeighting())\n",
    "\n",
    "import pprint\n",
    "for w in word_freq.most_common(5):\n",
    "    pprint.pprint(transformed.get_neighbours(w[0], 5, CosSimilarity()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997883\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "with codecs.open('./data/c.clean.en/data', 'r', encoding='utf-8') as f:\n",
    "    en = f.read().lower().splitlines()\n",
    "\n",
    "with codecs.open('./data/c.clean.es/data', 'r', encoding='utf-8') as f:\n",
    "    es = f.read().lower().splitlines()\n",
    "\n",
    "with open('./data/en-es_aligned.intersect/data', 'r') as f:\n",
    "    align = f.read().splitlines()\n",
    "    \n",
    "# check shapes\n",
    "assert len(en) == len(es)\n",
    "assert len(en) == len(align)\n",
    "print(len(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lemmas = []\n",
    "es_lemmas = []\n",
    "for doc in en:\n",
    "    en_lemmas.append(' '.join(w.lemma_ for w in nlp(doc)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in es:\n",
    "    es_lemmas.append(' '.join(w.lemma_ for w in nlp(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the text based on POS\n",
    "en_tagged = []\n",
    "es_tagged = []\n",
    "for doc in en:\n",
    "    en_tagged.append(' '.join(w.text+'_'+w.pos_ for w in nlp(doc)))\n",
    "for doc in es:\n",
    "    en_tagged.append(' '.join(w.text+'_'+w.pos_ for w in nlp(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the frequency distribution for words\n",
    "from collections import Counter\n",
    "word_freq = Counter()\n",
    "for t in filtered:\n",
    "    word_freq.update(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only the 1550 most frequent words\n",
    "vocab = [x[0] for x in word_freq.most_common(1550)]\n",
    "assert len(vocab) == 1550\n",
    "\n",
    "corpus = zip(en, es, align)\n",
    "assert len(corpus) == len(en)\n",
    "len(corpus)\n",
    "\n",
    "from collections import defaultdict\n",
    "mm = defaultdict(int)\n",
    "\n",
    "for x,y,z in corpus:\n",
    "    zs = z.split(' ')\n",
    "    for words in zs:\n",
    "        try:\n",
    "            mm[x.split()[int(words.split('-')[0])],\n",
    "               y.split()[int(words.split('-')[1])]] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "en = []\n",
    "es = []\n",
    "count = []\n",
    "for k,v in mm.items():\n",
    "    en.append(k[0])\n",
    "    es.append(k[1])\n",
    "    count.append(v)\n",
    "\n",
    "assert len(en) == len(es)\n",
    "assert len(en) == len(count)\n",
    "\n",
    "sm = pd.DataFrame([en,es,count]).T\n",
    "print(sm.head())\n",
    "\n",
    "sm.to_csv('parallel.sm', index=False, sep=' ', header=False)\n",
    "\n",
    "with open('parallel.rows',\"w\") as o:\n",
    "    for word in en:\n",
    "        o.write(word)\n",
    "        o.write('\\n')\n",
    "\n",
    "with open('parallel.cols',\"w\") as o:\n",
    "    for word in es:\n",
    "        o.write(word)\n",
    "        o.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel\n",
    "from composes.semantic_space.space import Space\n",
    "\n",
    "#create a space from co-occurrence counts in sparse format\n",
    "parallel = Space.build(data = \"./parallel.sm\",\n",
    "                       rows = \"./parallel.cols\",\n",
    "                       cols = \"./parallel.rows\",\n",
    "                       format = \"sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the frequency distribution for words\n",
    "from collections import Counter\n",
    "wf = Counter()\n",
    "for t in es:\n",
    "    wf.update(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 4223),\n",
       " (',', 2956),\n",
       " ('que', 2868),\n",
       " ('a', 2563),\n",
       " ('se', 2518),\n",
       " ('la', 2497),\n",
       " ('los', 2431),\n",
       " ('las', 2220),\n",
       " ('en', 2168),\n",
       " ('no', 1869),\n",
       " ('una', 1645),\n",
       " ('un', 1497),\n",
       " ('el', 1469),\n",
       " ('con', 1343),\n",
       " ('para', 955),\n",
       " ('m\\xc3\\xa1s', 913),\n",
       " ('y', 872),\n",
       " ('por', 796),\n",
       " ('contra', 778),\n",
       " ('es', 691),\n",
       " ('ha', 664),\n",
       " ('parte', 625),\n",
       " ('muy', 618),\n",
       " ('sobre', 618),\n",
       " ('bien', 591),\n",
       " ('sin', 571),\n",
       " ('entre', 564),\n",
       " ('lo', 563),\n",
       " ('su', 531),\n",
       " ('gran', 524),\n",
       " ('respecto', 518),\n",
       " ('hacer', 516),\n",
       " ('al', 515),\n",
       " ('del', 501),\n",
       " ('forma', 478),\n",
       " ('hecho', 447),\n",
       " ('poco', 442),\n",
       " ('hace', 441),\n",
       " ('como', 441),\n",
       " ('esta', 429),\n",
       " ('todo', 417),\n",
       " ('tambi\\xc3\\xa9n', 412),\n",
       " ('conseguir', 385),\n",
       " ('ser', 373),\n",
       " ('punto', 368),\n",
       " ('importante', 366),\n",
       " ('posible', 364),\n",
       " ('est\\xc3\\xa1', 364),\n",
       " ('tiene', 363),\n",
       " ('son', 356)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 most common\n",
    "for w in wf.most_common()[-50:-45]:\n",
    "    try:\n",
    "        pprint.pprint(parallel.get_neighbours(w[0], 5, CosSimilarity()))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('well-armed', 1.0),\n",
       " ('well-kept', 1.0),\n",
       " ('well-hidden', 1.0),\n",
       " ('welldefined', 1.0),\n",
       " ('bien', 1.0),\n",
       " ('well-anchored', 1.0),\n",
       " ('well-sustained', 1.0),\n",
       " ('well-oriented', 1.0),\n",
       " ('well-reasoned', 1.0),\n",
       " ('well-laid', 1.0),\n",
       " ('well-identified', 1.0),\n",
       " ('well-mapped', 1.0),\n",
       " ('well-spaced', 1.0),\n",
       " ('well-clothed', 1.0),\n",
       " ('well-applied', 1.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel.get_neighbours('bien', 15, CosSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('housecleaning', 1.0),\n",
       " ('house-by-house', 1.0),\n",
       " ('homewards', 1.0),\n",
       " ('home-worker', 1.0),\n",
       " ('house-label', 1.0),\n",
       " ('homemakers', 1.0),\n",
       " ('homebuyers', 1.0),\n",
       " ('house-to-house', 1.0),\n",
       " ('home-spun', 1.0),\n",
       " ('home-to-home', 1.0),\n",
       " ('casa', 0.9999999999999999),\n",
       " ('housewives', 0.9958705948858223),\n",
       " ('housewife', 0.9901475429766743),\n",
       " ('marries', 0.9486832980505138),\n",
       " ('housekeepers', 0.8944271909999159)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel.get_neighbours('casa', 15, CosSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tinto', 1.0),\n",
       " ('red', 0.057016996594512454),\n",
       " ('out', 0.0),\n",
       " ('euro-apathy', 0.0),\n",
       " ('mrs', 0.0),\n",
       " ('danger', 0.0),\n",
       " ('discouraging', 0.0),\n",
       " ('allow', 0.0),\n",
       " ('settings', 0.0),\n",
       " ('readjustments', 0.0),\n",
       " ('transposing', 0.0),\n",
       " ('inter-pillar', 0.0),\n",
       " ('achieve', 0.0),\n",
       " ('securing', 0.0),\n",
       " ('willing', 0.0)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel.get_neighbours('tinto', 15, CosSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deliberados', 0.0),\n",
       " ('euroapat\\xc3\\xada', 0.0),\n",
       " ('eluned', 0.0),\n",
       " ('amenazados', 0.0),\n",
       " ('desalentadora', 0.0),\n",
       " ('autoricemos', 0.0),\n",
       " ('escenarios', 0.0),\n",
       " ('dispuesta', 0.0),\n",
       " ('traslaci\\xc3\\xb3n', 0.0),\n",
       " ('entre', 0.0),\n",
       " ('conseguirla', 0.0),\n",
       " ('logros', 0.0),\n",
       " ('desean', 0.0),\n",
       " ('adhieran', 0.0),\n",
       " ('respetad\\xc3\\xadsima', 0.0)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel.get_neighbours('hacer', 15, CosSimilarity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import spacy\n",
    "\n",
    "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "king = nlp.vocab[u'king']\n",
    "man = nlp.vocab[u'man']\n",
    "woman = nlp.vocab[u'woman']\n",
    "\n",
    "result = king.vector - man.vector + woman.vector\n",
    "\n",
    "vocabulary = [w for w in nlp.vocab if w.has_vector and w.orth_.islower() and w.lower_ not in ['king','man','woman']]\n",
    "\n",
    "vocabulary.sort(key=lambda w: cosine(w.vector, result))\n",
    "vocabulary.reverse()\n",
    "\n",
    "print(vocabulary[0].orth_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
